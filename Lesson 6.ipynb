{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lesson 6\n",
    "\n",
    "\n",
    "Things we will learn in this lesson:\n",
    "\n",
    "    - We will be moving ahead from single hiddenlayer network to deep neural network building \n",
    "    - How to automate Deep neural network code creation with too many hidden layers?\n",
    "    - How to apply regularization using \"DROPOUT\"\n",
    "    - differnt types of initialization method in pytorch\n",
    "    - Batch normalization in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network\n",
    "\n",
    "Adding more neurons in a single hidden layer would result in overfiltting\n",
    "\n",
    "To overcome this we would add more hidden layer and hence would improve the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train  = datasets.MNIST(root ='./',train=True, download=False,transform=transforms.ToTensor())\n",
    "test = datasets.MNIST(root ='./',train=True, download=False,transform=transforms.ToTensor())\n",
    "\n",
    "train_dataset = DataLoader(dataset=train, batch_size=100)\n",
    "test_dataset = DataLoader(dataset=test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(Net,self).__init__()\n",
    "        self.Linear1 = nn.Linear(D_in, H1)\n",
    "        self.Linear2 = nn.Linear(H1, H2)\n",
    "        self.Linear3 = nn.Linear(H2, D_out)\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.Linear1(x))\n",
    "        x = torch.relu(self.Linear2(x))\n",
    "        x = self.Linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(784, 100, 100, 10)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost at the end of epoch 1260.1948161125183 is 0\n",
      "The cost at the end of epoch 572.1300027370453 is 1\n",
      "The cost at the end of epoch 311.073055498302 is 2\n",
      "The cost at the end of epoch 249.4968118444085 is 3\n",
      "The cost at the end of epoch 222.22233516722918 is 4\n",
      "The cost at the end of epoch 205.63527696952224 is 5\n",
      "The cost at the end of epoch 193.45121706277132 is 6\n",
      "The cost at the end of epoch 183.55334806069732 is 7\n",
      "The cost at the end of epoch 175.06302646175027 is 8\n",
      "The cost at the end of epoch 167.50499663501978 is 9\n"
     ]
    }
   ],
   "source": [
    "##Training\n",
    "for epoch in range(10):\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(train_dataset): \n",
    "        optimizer.zero_grad()\n",
    "        yhat = model.forward(x.view(-1, 28 * 28))\n",
    "        loss_ = loss(yhat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_.backward()\n",
    "        total = total + loss_.data.item()\n",
    "        optimizer.step()\n",
    "    print('The cost at the end of epoch {} is {}'.format(total, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for (x,y) in test_dataset:\n",
    "    yhat = model.forward(x.view(-1,784))\n",
    "    _,pred = torch.max(yhat,1)\n",
    "    #print(pred,y)\n",
    "    correct = correct + ((pred == y).sum().item())\n",
    "    #print(correct)\n",
    "\n",
    "#print(correct)\n",
    "accuracy = (correct/len(test)) *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to automate Deep neural network code creation with too many hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi layers need not be defined by hand, it can be easily automated using **nn.ModuleList()**\n",
    "\n",
    "Iteration can be then used to define based on the hidden layer neuron parameter as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, Layer_list):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i,j in zip(Layer_list,Layer_list[1:]):\n",
    "            if i != len(len(self.hidden))-2:\n",
    "                self.hidden.append(torch.relu(nn.Linear(i,j)))\n",
    "            else:\n",
    "                self.hidden.append(nn.Linear(i,j))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- drop out is a regularization technique\n",
    "\n",
    "-- we apply dropout during training\n",
    "\n",
    "-- we turn off drop out during test/validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(Net,self).__init__()\n",
    "        self.Linear1 = nn.Linear(D_in, H1)\n",
    "        self.Linear2 = nn.Linear(H1, H2)\n",
    "        self.Linear3 = nn.Linear(H2, D_out)\n",
    "        self.drop = nn.Dropout(0.2) #this line will drop 20% of the neurons from the layer output\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.Linear1(x))\n",
    "        x = self.drop(0.2)\n",
    "        x = torch.relu(self.Linear2(x))\n",
    "        x = self.drop(0.2)\n",
    "        x = self.Linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use dropout there is a caution\n",
    "\n",
    "-- while training the parameter has to be set as **model.train()** :\n",
    "    \n",
    "    this command will set the dropout function enables during the training process\n",
    "\n",
    "-- while testing the paramter has to be set as **model.eval()** :\n",
    "    \n",
    "    this command will set the dropout function disabled during tetsing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization methods in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization is very important, since the initial weights that are assigned as selected in random\n",
    "\n",
    "Since the values are selected in random, the correposding forward pass operation with activation might result in a huge value.\n",
    "\n",
    "Thereby causing the derivative to be very close to zero (sigmoid/tanh).This would clearly lead to **Vanishing Gradient Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: thus the range of random value distribution shoulb be intutively sleected\n",
    "    \n",
    "    The range is fixed between -1/(num of neurons) to +1/(num of neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default method:\n",
    "\n",
    ">> -1/(suqare root (num of neurons Lin))   to +1/(suqare root (num of neurons Lin)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier method:\n",
    "\n",
    ">> -square root(6)/(square root of Lin + Lout)  to  +square root(6)/(square root of Lin + Lout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Linear = nn.Linear(in_,out_)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(Linear.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### He method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Linear = nn.Linear(in_,out_)\n",
    "\n",
    "torch.nn.init.kaiming_uniform_(Linear.weight, nonlinearity='relu') #specially used when RELU is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum parameter to avoid saddle point/local minima in optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim.SGD(model.paramters, lr=0.01, momentum=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of every layer might have a differnt scale of values and hence it is very important to normalize them\n",
    "\n",
    "\n",
    "Batch normalization is used for this\n",
    "\n",
    ">> z-mean/square root(std.dev z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(Net,self).__init__()\n",
    "        self.Linear1 = nn.Linear(D_in, H1)\n",
    "        self.Linear2 = nn.Linear(H1, H2)\n",
    "        self.Linear3 = nn.Linear(H2, D_out)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(H1)\n",
    "        self.bn2 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.bn1(self.Linear1(x)))\n",
    "        x = torch.relu(self.bn2(self.Linear2(x)))\n",
    "        x = self.Linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
